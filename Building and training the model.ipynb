{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data from the data processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_train\n",
    "%store -r x_test\n",
    "%store -r y_train\n",
    "%store -r y_test\n",
    "%store -r yy\n",
    "%store -r le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Deep Learning frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the neural network. The Sequential Model is a plain stack of layers where there is one input tensor and one output tensor. The 256 is the number of nodes in each layer.\n",
    "\n",
    "Layer 1: Each sample has 40 MFCCs so the input shape is 40. The relu activation is the functiom that gives out the final value for each neuron/node. Relu stands for Rectified Linear Unit and is a common activation function for DL classification problems. The dropout is 0.5 to randomly exclude nodes from each epoch to create better generalisation and less overfitting.  \n",
    "\n",
    "Layer 2: Is the hidden layer, with the same structure as the first.\n",
    "\n",
    "Layer 3: Is the output layer, with 2 nodes; one for emergency and one for non emergency. the activation is Softmax, which makes the probability sum up to 1. So the two nodes added together must equal one. This is good for probability problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical crossentropy is used for classifciation where there are more than one output labels. Essentially this gives the model a score of how it is performing. The lower the score the more accurate the predicitons.\n",
    "The accuracy metric shows the accuracy on the validation data. \n",
    "The Adam optimizer is derived from Adaptive Moment Estimation. it is an extension to stochastic gradient descent to update network weights based off the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               10496     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 76,802\n",
      "Trainable params: 76,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to evaluate the test data before the network has been trained. This will show how training the model improves the predictions. (If the model has been trained in this session the accuracy will be high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training accuracy: 37.3786%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by fitting it to the training data. Go through 30 iterations to train the model. The number of epochs is largely decided by the user and should really  be how many it takes to improve accuracy before stabilising.\n",
    "\n",
    "Batch size is definining how many samples go through the netwrok before it is trained. So here, send through 32 samples, then update the network etc. This requires less computational power as the network is not being updated for every sample and the network trains faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1645 samples, validate on 412 samples\n",
      "Epoch 1/30\n",
      "1645/1645 [==============================] - 1s 654us/step - loss: 5.8176 - accuracy: 0.7878 - val_loss: 1.7352 - val_accuracy: 0.8665\n",
      "Epoch 2/30\n",
      "1645/1645 [==============================] - 0s 232us/step - loss: 1.9685 - accuracy: 0.8395 - val_loss: 0.7938 - val_accuracy: 0.8738\n",
      "Epoch 3/30\n",
      "1645/1645 [==============================] - 0s 228us/step - loss: 1.4192 - accuracy: 0.8553 - val_loss: 0.6871 - val_accuracy: 0.8689\n",
      "Epoch 4/30\n",
      "1645/1645 [==============================] - 0s 222us/step - loss: 0.9171 - accuracy: 0.8578 - val_loss: 0.4564 - val_accuracy: 0.8859\n",
      "Epoch 5/30\n",
      "1645/1645 [==============================] - 0s 233us/step - loss: 0.7487 - accuracy: 0.8626 - val_loss: 0.3474 - val_accuracy: 0.8835\n",
      "Epoch 6/30\n",
      "1645/1645 [==============================] - 0s 244us/step - loss: 0.4801 - accuracy: 0.8766 - val_loss: 0.2879 - val_accuracy: 0.8883\n",
      "Epoch 7/30\n",
      "1645/1645 [==============================] - 0s 236us/step - loss: 0.4950 - accuracy: 0.8687 - val_loss: 0.2960 - val_accuracy: 0.8883\n",
      "Epoch 8/30\n",
      "1645/1645 [==============================] - 0s 240us/step - loss: 0.3931 - accuracy: 0.8790 - val_loss: 0.3000 - val_accuracy: 0.8883\n",
      "Epoch 9/30\n",
      "1645/1645 [==============================] - 0s 244us/step - loss: 0.3581 - accuracy: 0.8729 - val_loss: 0.3363 - val_accuracy: 0.8981\n",
      "Epoch 10/30\n",
      "1645/1645 [==============================] - 0s 249us/step - loss: 0.3244 - accuracy: 0.8857 - val_loss: 0.2925 - val_accuracy: 0.8883\n",
      "Epoch 11/30\n",
      "1645/1645 [==============================] - 0s 258us/step - loss: 0.3146 - accuracy: 0.8875 - val_loss: 0.2978 - val_accuracy: 0.8835\n",
      "Epoch 12/30\n",
      "1645/1645 [==============================] - 0s 251us/step - loss: 0.2798 - accuracy: 0.8930 - val_loss: 0.2954 - val_accuracy: 0.8859\n",
      "Epoch 13/30\n",
      "1645/1645 [==============================] - 0s 255us/step - loss: 0.2908 - accuracy: 0.8985 - val_loss: 0.2794 - val_accuracy: 0.9029\n",
      "Epoch 14/30\n",
      "1645/1645 [==============================] - 0s 240us/step - loss: 0.2733 - accuracy: 0.9015 - val_loss: 0.2766 - val_accuracy: 0.9102\n",
      "Epoch 15/30\n",
      "1645/1645 [==============================] - 0s 241us/step - loss: 0.2737 - accuracy: 0.9058 - val_loss: 0.2717 - val_accuracy: 0.9102\n",
      "Epoch 16/30\n",
      "1645/1645 [==============================] - 0s 253us/step - loss: 0.2752 - accuracy: 0.8924 - val_loss: 0.2733 - val_accuracy: 0.8932\n",
      "Epoch 17/30\n",
      "1645/1645 [==============================] - 0s 271us/step - loss: 0.2506 - accuracy: 0.9027 - val_loss: 0.2678 - val_accuracy: 0.9053\n",
      "Epoch 18/30\n",
      "1645/1645 [==============================] - 0s 257us/step - loss: 0.2419 - accuracy: 0.9094 - val_loss: 0.2577 - val_accuracy: 0.8981\n",
      "Epoch 19/30\n",
      "1645/1645 [==============================] - 0s 258us/step - loss: 0.2378 - accuracy: 0.9119 - val_loss: 0.2565 - val_accuracy: 0.9005\n",
      "Epoch 20/30\n",
      "1645/1645 [==============================] - 0s 261us/step - loss: 0.2342 - accuracy: 0.9155 - val_loss: 0.2628 - val_accuracy: 0.9005\n",
      "Epoch 21/30\n",
      "1645/1645 [==============================] - 0s 265us/step - loss: 0.2463 - accuracy: 0.9137 - val_loss: 0.2413 - val_accuracy: 0.9029\n",
      "Epoch 22/30\n",
      "1645/1645 [==============================] - 0s 262us/step - loss: 0.2263 - accuracy: 0.9143 - val_loss: 0.2381 - val_accuracy: 0.9126\n",
      "Epoch 23/30\n",
      "1645/1645 [==============================] - 0s 270us/step - loss: 0.2014 - accuracy: 0.9204 - val_loss: 0.2252 - val_accuracy: 0.9053\n",
      "Epoch 24/30\n",
      "1645/1645 [==============================] - 0s 270us/step - loss: 0.2108 - accuracy: 0.9191 - val_loss: 0.2339 - val_accuracy: 0.9126\n",
      "Epoch 25/30\n",
      "1645/1645 [==============================] - 0s 271us/step - loss: 0.1958 - accuracy: 0.9228 - val_loss: 0.2196 - val_accuracy: 0.9175\n",
      "Epoch 26/30\n",
      "1645/1645 [==============================] - 0s 266us/step - loss: 0.2064 - accuracy: 0.9325 - val_loss: 0.2300 - val_accuracy: 0.9223\n",
      "Epoch 27/30\n",
      "1645/1645 [==============================] - 0s 269us/step - loss: 0.2081 - accuracy: 0.9234 - val_loss: 0.2286 - val_accuracy: 0.9175\n",
      "Epoch 28/30\n",
      "1645/1645 [==============================] - 0s 299us/step - loss: 0.2029 - accuracy: 0.9295 - val_loss: 0.2355 - val_accuracy: 0.9199\n",
      "Epoch 29/30\n",
      "1645/1645 [==============================] - 0s 295us/step - loss: 0.2020 - accuracy: 0.9210 - val_loss: 0.2360 - val_accuracy: 0.9102\n",
      "Epoch 30/30\n",
      "1645/1645 [==============================] - 0s 293us/step - loss: 0.2014 - accuracy: 0.9246 - val_loss: 0.2210 - val_accuracy: 0.9150\n",
      "Training completed in time:  0:00:13.808711\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 30\n",
    "num_batch_size = 32\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on both the train set and the test set. This should give an indication of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9410334229469299\n",
      "Testing Accuracy:  0.9150485396385193\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_model')\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the classifier, use the same extract features function as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import numpy as np \n",
    "\n",
    "def extract_feature(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None, None\n",
    "\n",
    "    return np.array([mfccsscaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(file_name):\n",
    "    prediction_feature = extract_feature(file_name) \n",
    "\n",
    "    predicted_vector = model.predict_classes(prediction_feature)\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
    "\n",
    "    predicted_proba_vector = model.predict_proba(prediction_feature) \n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: non_emergency \n",
      "\n",
      "emergency \t\t :  0.00110092386603355407714843750000\n",
      "non_emergency \t\t :  0.99889910221099853515625000000000\n"
     ]
    }
   ],
   "source": [
    "emergency_test_dataset_path = '../Datasets/test/train_balanced/nonEmergency/11.wav'\n",
    "print_prediction(emergency_test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'history' (History)\n"
     ]
    }
   ],
   "source": [
    "%store history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
